{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.15","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"29268cbbf3484e21a37bf65de4f6c4ee":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef81085bff5c455c87415289f73b54f9","placeholder":"​","style":"IPY_MODEL_4936affa4d894df3b4afaf06142dd972","value":" 0/10 [00:00&lt;?, ?it/s]"}},"2b21ef9a47c84604a9c7d118332ccf34":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"45435f8d207c44c9b3611351ba4ee55f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e69e159c8d8b46cab71ff1c42db3c385","IPY_MODEL_7214f669b7d64e039033850ad9012e1a","IPY_MODEL_29268cbbf3484e21a37bf65de4f6c4ee"],"layout":"IPY_MODEL_e44b6bcef4364403aef894d9b21bfb9f"}},"4936affa4d894df3b4afaf06142dd972":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6da04b76addd42a4b4044955559d992f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7214f669b7d64e039033850ad9012e1a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_2b21ef9a47c84604a9c7d118332ccf34","max":10,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e0628ca05aa94710916516a9bc024b75","value":0}},"e0628ca05aa94710916516a9bc024b75":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e3d6b8cf2ef5410690f6a8c9c04221c7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e44b6bcef4364403aef894d9b21bfb9f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e69e159c8d8b46cab71ff1c42db3c385":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6da04b76addd42a4b4044955559d992f","placeholder":"​","style":"IPY_MODEL_e3d6b8cf2ef5410690f6a8c9c04221c7","value":"  0%"}},"ef81085bff5c455c87415289f73b54f9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":9617380,"sourceType":"datasetVersion","datasetId":5869363},{"sourceId":133931,"sourceType":"modelInstanceVersion","modelInstanceId":113257,"modelId":136581}],"dockerImageVersionId":30788,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Source**\n\n\n\n\n[YouTube](https://youtu.be/awyWND506NY) , [GitHub\n](https://github.com/AarohiSingla/Image-Classification-Using-Vision-transformer)\n\ndataset : https://www.kaggle.com/datasets/l33tc0d3r/indian-food-classification","metadata":{"id":"f1N8tkjjazXC"}},{"cell_type":"markdown","source":"# **Import and Instalization**","metadata":{"id":"jqliht5Vsn6Y"}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport torchvision\nimport os\nfrom torch import nn\nfrom torchvision import transforms\n\nimport tensorflow as tf\nfrom tensorflow import keras","metadata":{"id":"_3_vd3GTWh2R","execution":{"iopub.status.busy":"2024-10-13T23:13:14.226531Z","iopub.execute_input":"2024-10-13T23:13:14.227584Z","iopub.status.idle":"2024-10-13T23:13:52.732965Z","shell.execute_reply.started":"2024-10-13T23:13:14.227542Z","shell.execute_reply":"2024-10-13T23:13:52.731780Z"},"trusted":true},"outputs":[{"name":"stderr","text":"WARNING: Logging before InitGoogle() is written to STDERR\nE0000 00:00:1728861224.447248      13 common_lib.cc:798] Could not set metric server port: INVALID_ARGUMENT: Could not find SliceBuilder port 8471 in any of the 0 ports provided in `tpu_process_addresses`=\"local\"\n=== Source Location Trace: === \nlearning/45eac/tfrc/runtime/common_lib.cc:479\nD1013 23:13:44.455643440      13 config.cc:196]                        gRPC EXPERIMENT call_status_override_on_cancellation   OFF (default:OFF)\nD1013 23:13:44.455663276      13 config.cc:196]                        gRPC EXPERIMENT call_v3                                OFF (default:OFF)\nD1013 23:13:44.455667421      13 config.cc:196]                        gRPC EXPERIMENT canary_client_privacy                  ON  (default:ON)\nD1013 23:13:44.455670367      13 config.cc:196]                        gRPC EXPERIMENT capture_base_context                   ON  (default:ON)\nD1013 23:13:44.455673247      13 config.cc:196]                        gRPC EXPERIMENT client_idleness                        ON  (default:ON)\nD1013 23:13:44.455675987      13 config.cc:196]                        gRPC EXPERIMENT client_privacy                         ON  (default:ON)\nD1013 23:13:44.455678736      13 config.cc:196]                        gRPC EXPERIMENT dapper_request_wire_size               OFF (default:OFF)\nD1013 23:13:44.455681430      13 config.cc:196]                        gRPC EXPERIMENT empty_experiment                       OFF (default:OFF)\nD1013 23:13:44.455686867      13 config.cc:196]                        gRPC EXPERIMENT event_engine_client                    OFF (default:OFF)\nD1013 23:13:44.455689774      13 config.cc:196]                        gRPC EXPERIMENT event_engine_dns                       ON  (default:ON)\nD1013 23:13:44.455692447      13 config.cc:196]                        gRPC EXPERIMENT event_engine_listener                  ON  (default:ON)\nD1013 23:13:44.455695066      13 config.cc:196]                        gRPC EXPERIMENT free_large_allocator                   OFF (default:OFF)\nD1013 23:13:44.455697637      13 config.cc:196]                        gRPC EXPERIMENT google_no_envelope_resolver            OFF (default:OFF)\nD1013 23:13:44.455700211      13 config.cc:196]                        gRPC EXPERIMENT http2_stats_fix                        OFF (default:OFF)\nD1013 23:13:44.455702767      13 config.cc:196]                        gRPC EXPERIMENT keepalive_fix                          OFF (default:OFF)\nD1013 23:13:44.455705336      13 config.cc:196]                        gRPC EXPERIMENT keepalive_server_fix                   ON  (default:ON)\nD1013 23:13:44.455708089      13 config.cc:196]                        gRPC EXPERIMENT loas_do_not_prefer_rekey_next_protocol OFF (default:OFF)\nD1013 23:13:44.455710910      13 config.cc:196]                        gRPC EXPERIMENT loas_prod_to_cloud_prefer_pfs_ciphers  OFF (default:OFF)\nD1013 23:13:44.455713529      13 config.cc:196]                        gRPC EXPERIMENT monitoring_experiment                  ON  (default:ON)\nD1013 23:13:44.455716205      13 config.cc:196]                        gRPC EXPERIMENT multiping                              OFF (default:OFF)\nD1013 23:13:44.455718788      13 config.cc:196]                        gRPC EXPERIMENT peer_state_based_framing               OFF (default:OFF)\nD1013 23:13:44.455721377      13 config.cc:196]                        gRPC EXPERIMENT pending_queue_cap                      ON  (default:ON)\nD1013 23:13:44.455724065      13 config.cc:196]                        gRPC EXPERIMENT pick_first_happy_eyeballs              ON  (default:ON)\nD1013 23:13:44.455726701      13 config.cc:196]                        gRPC EXPERIMENT promise_based_client_call              OFF (default:OFF)\nD1013 23:13:44.455729242      13 config.cc:196]                        gRPC EXPERIMENT promise_based_inproc_transport         OFF (default:OFF)\nD1013 23:13:44.455731791      13 config.cc:196]                        gRPC EXPERIMENT promise_based_server_call              OFF (default:OFF)\nD1013 23:13:44.455734438      13 config.cc:196]                        gRPC EXPERIMENT registered_method_lookup_in_transport  ON  (default:ON)\nD1013 23:13:44.455737074      13 config.cc:196]                        gRPC EXPERIMENT rfc_max_concurrent_streams             ON  (default:ON)\nD1013 23:13:44.455739846      13 config.cc:196]                        gRPC EXPERIMENT round_robin_delegate_to_pick_first     ON  (default:ON)\nD1013 23:13:44.455743880      13 config.cc:196]                        gRPC EXPERIMENT rstpit                                 OFF (default:OFF)\nD1013 23:13:44.455746789      13 config.cc:196]                        gRPC EXPERIMENT schedule_cancellation_over_write       OFF (default:OFF)\nD1013 23:13:44.455749509      13 config.cc:196]                        gRPC EXPERIMENT server_privacy                         ON  (default:ON)\nD1013 23:13:44.455752364      13 config.cc:196]                        gRPC EXPERIMENT tcp_frame_size_tuning                  OFF (default:OFF)\nD1013 23:13:44.455755001      13 config.cc:196]                        gRPC EXPERIMENT tcp_rcv_lowat                          OFF (default:OFF)\nD1013 23:13:44.455757627      13 config.cc:196]                        gRPC EXPERIMENT trace_record_callops                   OFF (default:OFF)\nD1013 23:13:44.455760221      13 config.cc:196]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size    OFF (default:OFF)\nD1013 23:13:44.455762729      13 config.cc:196]                        gRPC EXPERIMENT v3_backend_metric_filter               OFF (default:OFF)\nD1013 23:13:44.455765321      13 config.cc:196]                        gRPC EXPERIMENT v3_channel_idle_filters                ON  (default:ON)\nD1013 23:13:44.455768051      13 config.cc:196]                        gRPC EXPERIMENT v3_compression_filter                  ON  (default:ON)\nD1013 23:13:44.455770715      13 config.cc:196]                        gRPC EXPERIMENT v3_server_auth_filter                  OFF (default:OFF)\nD1013 23:13:44.455773323      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_clears_time_cache      OFF (default:OFF)\nD1013 23:13:44.455775899      13 config.cc:196]                        gRPC EXPERIMENT work_serializer_dispatch               OFF (default:OFF)\nD1013 23:13:44.455778525      13 config.cc:196]                        gRPC EXPERIMENT write_size_cap                         ON  (default:ON)\nD1013 23:13:44.455781204      13 config.cc:196]                        gRPC EXPERIMENT write_size_policy                      ON  (default:ON)\nD1013 23:13:44.455783881      13 config.cc:196]                        gRPC EXPERIMENT wrr_delegate_to_pick_first             ON  (default:ON)\nI1013 23:13:44.456007333      13 ev_epoll1_linux.cc:123]               grpc epoll fd: 59\nD1013 23:13:44.456023810      13 ev_posix.cc:113]                      Using polling engine: epoll1\nD1013 23:13:44.466346593      13 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\nD1013 23:13:44.466359291      13 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\nD1013 23:13:44.466368710      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\nD1013 23:13:44.466372943      13 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\nD1013 23:13:44.466376677      13 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\nD1013 23:13:44.466380181      13 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin\"\nD1013 23:13:44.466412775      13 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\nD1013 23:13:44.466430871      13 dns_resolver_plugin.cc:43]            Using EventEngine dns resolver\nD1013 23:13:44.466449930      13 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\nD1013 23:13:44.466475077      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\nD1013 23:13:44.466484341      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\nD1013 23:13:44.466488450      13 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\nD1013 23:13:44.466493550      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\nD1013 23:13:44.466497171      13 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\nD1013 23:13:44.466501145      13 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\nD1013 23:13:44.466508433      13 certificate_provider_registry.cc:33]  registering certificate provider factory for \"file_watcher\"\nD1013 23:13:44.466543250      13 channel_init.cc:157]                  Filter server-auth not registered, but is referenced in the after clause of grpc-server-authz when building channel stack SERVER_CHANNEL\nI1013 23:13:44.468322385      13 ev_epoll1_linux.cc:359]               grpc epoll fd: 61\nI1013 23:13:44.469621011      13 tcp_socket_utils.cc:689]              Disabling AF_INET6 sockets because ::1 is not available.\nI1013 23:13:44.490086946     105 socket_utils_common_posix.cc:452]     Disabling AF_INET6 sockets because ::1 is not available.\nI1013 23:13:44.490155298     105 socket_utils_common_posix.cc:379]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\nE1013 23:13:44.496068523      13 oauth2_credentials.cc:238]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-10-13T23:13:44.496051023+00:00\"}\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"pip install torchinfo","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1aznOJDJWpQg","outputId":"ef01671d-e135-4df8-93ae-73bdbff12a34","execution":{"iopub.status.busy":"2024-10-13T23:13:52.734643Z","iopub.execute_input":"2024-10-13T23:13:52.735290Z","iopub.status.idle":"2024-10-13T23:13:57.089888Z","shell.execute_reply.started":"2024-10-13T23:13:52.735257Z","shell.execute_reply":"2024-10-13T23:13:57.088832Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Collecting torchinfo\n  Downloading torchinfo-1.8.0-py3-none-any.whl (23 kB)\nInstalling collected packages: torchinfo\nSuccessfully installed torchinfo-1.8.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"# **Access to dataset**","metadata":{"id":"dJHmuzq8seI1"}},{"cell_type":"code","source":"train_dir = '/kaggle/input/indian-food-image/train'\ntest_dir = '/kaggle/input/indian-food-image/test'","metadata":{"id":"W99NyRVJWxKq","execution":{"iopub.status.busy":"2024-10-13T23:13:57.091216Z","iopub.execute_input":"2024-10-13T23:13:57.091487Z","iopub.status.idle":"2024-10-13T23:13:57.095945Z","shell.execute_reply.started":"2024-10-13T23:13:57.091460Z","shell.execute_reply":"2024-10-13T23:13:57.095271Z"},"trusted":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ndevice","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"ejm3sQdVWilb","outputId":"2dd9c1b1-6f99-4ceb-ac88-8f7341b0a57d","execution":{"iopub.status.busy":"2024-10-13T23:13:57.097823Z","iopub.execute_input":"2024-10-13T23:13:57.098097Z","iopub.status.idle":"2024-10-13T23:13:57.111931Z","shell.execute_reply.started":"2024-10-13T23:13:57.098073Z","shell.execute_reply":"2024-10-13T23:13:57.111340Z"},"trusted":true},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'cpu'"},"metadata":{}}],"execution_count":5},{"cell_type":"markdown","source":"# **The functions we will use**","metadata":{"id":"48nBgsh-ty8K"}},{"cell_type":"code","source":"\"\"\"\nA series of helper functions used throughout the course.\n\nIf a function gets defined once and could be used over and over, it'll go in here.\n\"\"\"\nimport torch\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nfrom torch import nn\nimport os\nimport zipfile\nfrom pathlib import Path\nimport requests\nimport os\n\n\n\n# Plot linear data or training and test and predictions (optional)\ndef plot_predictions(\n    train_data, train_labels, test_data, test_labels, predictions=None\n):\n    \"\"\"\n  Plots linear training data and test data and compares predictions.\n  \"\"\"\n    plt.figure(figsize=(10, 7))\n\n    # Plot training data in blue\n    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n\n    # Plot test data in green\n    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n\n    if predictions is not None:\n        # Plot the predictions in red (predictions were made on the test data)\n        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n\n    # Show the legend\n    plt.legend(prop={\"size\": 14})\n\n\n# Calculate accuracy (a classification metric)\ndef accuracy_fn(y_true, y_pred):\n    \"\"\"Calculates accuracy between truth labels and predictions.\n\n    Args:\n        y_true (torch.Tensor): Truth labels for predictions.\n        y_pred (torch.Tensor): Predictions to be compared to predictions.\n\n    Returns:\n        [torch.float]: Accuracy value between y_true and y_pred, e.g. 78.45\n    \"\"\"\n    correct = torch.eq(y_true, y_pred).sum().item()\n    acc = (correct / len(y_pred)) * 100\n    return acc\n\n\ndef print_train_time(start, end, device=None):\n    \"\"\"Prints difference between start and end time.\n\n    Args:\n        start (float): Start time of computation (preferred in timeit format).\n        end (float): End time of computation.\n        device ([type], optional): Device that compute is running on. Defaults to None.\n\n    Returns:\n        float: time between start and end in seconds (higher is longer).\n    \"\"\"\n    total_time = end - start\n    print(f\"\\nTrain time on {device}: {total_time:.3f} seconds\")\n    return total_time\n\n\n# Plot loss curves of a model\ndef plot_loss_curves(results):\n    \"\"\"Plots training curves of a results dictionary.\n\n    Args:\n        results (dict): dictionary containing list of values, e.g.\n            {\"train_loss\": [...],\n             \"train_acc\": [...],\n             \"test_loss\": [...],\n             \"test_acc\": [...]}\n    \"\"\"\n    loss = results[\"train_loss\"]\n    test_loss = results[\"test_loss\"]\n\n    accuracy = results[\"train_acc\"]\n    test_accuracy = results[\"test_acc\"]\n\n    epochs = range(len(results[\"train_loss\"]))\n\n    plt.figure(figsize=(15, 7))\n\n    # Plot loss\n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, loss, label=\"train_loss\")\n    plt.plot(epochs, test_loss, label=\"test_loss\")\n    plt.title(\"Loss\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n    # Plot accuracy\n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, accuracy, label=\"train_accuracy\")\n    plt.plot(epochs, test_accuracy, label=\"test_accuracy\")\n    plt.title(\"Accuracy\")\n    plt.xlabel(\"Epochs\")\n    plt.legend()\n\n\n# Pred and plot image function from notebook 04\n# See creation: https://www.learnpytorch.io/04_pytorch_custom_datasets/#113-putting-custom-image-prediction-together-building-a-function\nfrom typing import List\nimport torchvision\n\n\ndef pred_and_plot_image(\n    model: torch.nn.Module,\n    image_path: str,\n    class_names: List[str] = None,\n    transform=None,\n    device: torch.device = \"cuda\" if torch.cuda.is_available() else \"cpu\",\n):\n    \"\"\"Makes a prediction on a target image with a trained model and plots the image.\n\n    Args:\n        model (torch.nn.Module): trained PyTorch image classification model.\n        image_path (str): filepath to target image.\n        class_names (List[str], optional): different class names for target image. Defaults to None.\n        transform (_type_, optional): transform of target image. Defaults to None.\n        device (torch.device, optional): target device to compute on. Defaults to \"cuda\" if torch.cuda.is_available() else \"cpu\".\n\n    Returns:\n        Matplotlib plot of target image and model prediction as title.\n\n    Example usage:\n        pred_and_plot_image(model=model,\n                            image=\"some_image.jpeg\",\n                            class_names=[\"class_1\", \"class_2\", \"class_3\"],\n                            transform=torchvision.transforms.ToTensor(),\n                            device=device)\n    \"\"\"\n\n    # 1. Load in image and convert the tensor values to float32\n    target_image = torchvision.io.read_image(str(image_path)).type(torch.float32)\n\n    # 2. Divide the image pixel values by 255 to get them between [0, 1]\n    target_image = target_image / 255.0\n\n    # 3. Transform if necessary\n    if transform:\n        target_image = transform(target_image)\n\n    # 4. Make sure the model is on the target device\n    model.to(device)\n\n    # 5. Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # Add an extra dimension to the image\n        target_image = target_image.unsqueeze(dim=0)\n\n        # Make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(target_image.to(device))\n\n    # 6. Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # 7. Convert prediction probabilities -> prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # 8. Plot the image alongside the prediction and prediction probability\n    plt.imshow(\n        target_image.squeeze().permute(1, 2, 0)\n    )  # make sure it's the right size for matplotlib\n    if class_names:\n        title = f\"Pred: {class_names[target_image_pred_label.cpu()]} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    else:\n        title = f\"Pred: {target_image_pred_label} | Prob: {target_image_pred_probs.max().cpu():.3f}\"\n    plt.title(title)\n    plt.axis(False)\n\ndef set_seeds(seed: int=42):\n    \"\"\"Sets random sets for torch operations.\n\n    Args:\n        seed (int, optional): Random seed to set. Defaults to 42.\n    \"\"\"\n    # Set the seed for general torch operations\n    torch.manual_seed(seed)\n    # Set the seed for CUDA torch operations (ones that happen on the GPU)\n    torch.cuda.manual_seed(seed)\n","metadata":{"id":"E3eEBjeuWYRp","execution":{"iopub.status.busy":"2024-10-13T23:13:57.112760Z","iopub.execute_input":"2024-10-13T23:13:57.112997Z","iopub.status.idle":"2024-10-13T23:13:57.131158Z","shell.execute_reply.started":"2024-10-13T23:13:57.112973Z","shell.execute_reply":"2024-10-13T23:13:57.130565Z"},"trusted":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"\"\"\"\nContains functions for training and testing a PyTorch model.\n\"\"\"\nimport torch\n\nfrom tqdm.auto import tqdm\nfrom typing import Dict, List, Tuple\n\ndef train_step(model: torch.nn.Module,\n               dataloader: torch.utils.data.DataLoader,\n               loss_fn: torch.nn.Module,\n               optimizer: torch.optim.Optimizer,\n               device: torch.device) -> Tuple[float, float]:\n    \"\"\"Trains a PyTorch model for a single epoch.\n\n    Turns a target PyTorch model to training mode and then\n    runs through all of the required training steps (forward\n    pass, loss calculation, optimizer step).\n\n    Args:\n    model: A PyTorch model to be trained.\n    dataloader: A DataLoader instance for the model to be trained on.\n    loss_fn: A PyTorch loss function to minimize.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A tuple of training loss and training accuracy metrics.\n    In the form (train_loss, train_accuracy). For example:\n\n    (0.1112, 0.8743)\n    \"\"\"\n    # Put model in train mode\n    model.train()\n\n    # Setup train loss and train accuracy values\n    train_loss, train_acc = 0, 0\n\n    # Loop through data loader data batches\n    for batch, (X, y) in enumerate(dataloader):\n        # Send data to target device\n        X, y = X.to(device), y.to(device)\n\n        # 1. Forward pass\n        y_pred = model(X)\n\n        # 2. Calculate  and accumulate loss\n        loss = loss_fn(y_pred, y)\n        train_loss += loss.item()\n\n        # 3. Optimizer zero grad\n        optimizer.zero_grad()\n\n        # 4. Loss backward\n        loss.backward()\n\n        # 5. Optimizer step\n        optimizer.step()\n\n        # Calculate and accumulate accuracy metric across all batches\n        y_pred_class = torch.argmax(torch.softmax(y_pred, dim=1), dim=1)\n        train_acc += (y_pred_class == y).sum().item()/len(y_pred)\n\n    # Adjust metrics to get average loss and accuracy per batch\n    train_loss = train_loss / len(dataloader)\n    train_acc = train_acc / len(dataloader)\n    return train_loss, train_acc\n\ndef test_step(model: torch.nn.Module,\n              dataloader: torch.utils.data.DataLoader,\n              loss_fn: torch.nn.Module,\n              device: torch.device) -> Tuple[float, float]:\n    \"\"\"Tests a PyTorch model for a single epoch.\n\n    Turns a target PyTorch model to \"eval\" mode and then performs\n    a forward pass on a testing dataset.\n\n    Args:\n    model: A PyTorch model to be tested.\n    dataloader: A DataLoader instance for the model to be tested on.\n    loss_fn: A PyTorch loss function to calculate loss on the test data.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A tuple of testing loss and testing accuracy metrics.\n    In the form (test_loss, test_accuracy). For example:\n\n    (0.0223, 0.8985)\n    \"\"\"\n    # Put model in eval mode\n    model.eval()\n\n    # Setup test loss and test accuracy values\n    test_loss, test_acc = 0, 0\n\n    # Turn on inference context manager\n    with torch.inference_mode():\n        # Loop through DataLoader batches\n        for batch, (X, y) in enumerate(dataloader):\n            # Send data to target device\n            X, y = X.to(device), y.to(device)\n\n            # 1. Forward pass\n            test_pred_logits = model(X)\n\n            # 2. Calculate and accumulate loss\n            loss = loss_fn(test_pred_logits, y)\n            test_loss += loss.item()\n\n            # Calculate and accumulate accuracy\n            test_pred_labels = test_pred_logits.argmax(dim=1)\n            test_acc += ((test_pred_labels == y).sum().item()/len(test_pred_labels))\n\n    # Adjust metrics to get average loss and accuracy per batch\n    test_loss = test_loss / len(dataloader)\n    test_acc = test_acc / len(dataloader)\n    return test_loss, test_acc\n\ndef en_train(model: torch.nn.Module,\n          train_dataloader: torch.utils.data.DataLoader,\n          test_dataloader: torch.utils.data.DataLoader,\n          optimizer: torch.optim.Optimizer,\n          loss_fn: torch.nn.Module,\n          epochs: int,\n          device: torch.device) -> Dict[str, List]:\n    \"\"\"Trains and tests a PyTorch model.\n\n    Passes a target PyTorch models through train_step() and test_step()\n    functions for a number of epochs, training and testing the model\n    in the same epoch loop.\n\n    Calculates, prints and stores evaluation metrics throughout.\n\n    Args:\n    model: A PyTorch model to be trained and tested.\n    train_dataloader: A DataLoader instance for the model to be trained on.\n    test_dataloader: A DataLoader instance for the model to be tested on.\n    optimizer: A PyTorch optimizer to help minimize the loss function.\n    loss_fn: A PyTorch loss function to calculate loss on both datasets.\n    epochs: An integer indicating how many epochs to train for.\n    device: A target device to compute on (e.g. \"cuda\" or \"cpu\").\n\n    Returns:\n    A dictionary of training and testing loss as well as training and\n    testing accuracy metrics. Each metric has a value in a list for\n    each epoch.\n    In the form: {train_loss: [...],\n              train_acc: [...],\n              test_loss: [...],\n              test_acc: [...]}\n    For example if training for epochs=2:\n             {train_loss: [2.0616, 1.0537],\n              train_acc: [0.3945, 0.3945],\n              test_loss: [1.2641, 1.5706],\n              test_acc: [0.3400, 0.2973]}\n    \"\"\"\n    # Create empty results dictionary\n    results = {\"train_loss\": [],\n               \"train_acc\": [],\n               \"test_loss\": [],\n               \"test_acc\": []\n    }\n\n    # Make sure model on target device\n    model.to(device)\n\n    # Loop through training and testing steps for a number of epochs\n    for epoch in tqdm(range(epochs)):\n        train_loss, train_acc = train_step(model=model,\n                                          dataloader=train_dataloader,\n                                          loss_fn=loss_fn,\n                                          optimizer=optimizer,\n                                          device=device)\n        test_loss, test_acc = test_step(model=model,\n          dataloader=test_dataloader,\n          loss_fn=loss_fn,\n          device=device)\n\n        # Print out what's happening\n        print(\n          f\"Epoch: {epoch+1} | \"\n          f\"train_loss: {train_loss:.4f} | \"\n          f\"train_acc: {train_acc:.4f} | \"\n          f\"test_loss: {test_loss:.4f} | \"\n          f\"test_acc: {test_acc:.4f}\"\n        )\n\n        # Update results dictionary\n        results[\"train_loss\"].append(train_loss)\n        results[\"train_acc\"].append(train_acc)\n        results[\"test_loss\"].append(test_loss)\n        results[\"test_acc\"].append(test_acc)\n\n    # Return the filled results at the end of the epochs\n    return results","metadata":{"id":"En8FvVJlXdO0","execution":{"iopub.status.busy":"2024-10-13T23:13:57.132002Z","iopub.execute_input":"2024-10-13T23:13:57.132243Z","iopub.status.idle":"2024-10-13T23:13:57.153544Z","shell.execute_reply.started":"2024-10-13T23:13:57.132220Z","shell.execute_reply":"2024-10-13T23:13:57.152849Z"},"trusted":true},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"\"\"\"\nUtility functions to make predictions.\n\nMain reference for code creation: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\n\"\"\"\nimport torch\nimport torchvision\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\n\nfrom typing import List, Tuple\n\nfrom PIL import Image\n\n# Set device\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Predict on a target image with a target model\n# Function created in: https://www.learnpytorch.io/06_pytorch_transfer_learning/#6-make-predictions-on-images-from-the-test-set\ndef pred_and_plot_image(\n    model: torch.nn.Module,\n    class_names: List[str],\n    image_path: str,\n    image_size: Tuple[int, int] = (518, 518),\n    transform: torchvision.transforms = None,\n    device: torch.device = device,\n):\n    \"\"\"Predicts on a target image with a target model.\n\n    Args:\n        model (torch.nn.Module): A trained (or untrained) PyTorch model to predict on an image.\n        class_names (List[str]): A list of target classes to map predictions to.\n        image_path (str): Filepath to target image to predict on.\n        image_size (Tuple[int, int], optional): Size to transform target image to. Defaults to (224, 224).\n        transform (torchvision.transforms, optional): Transform to perform on image. Defaults to None which uses ImageNet normalization.\n        device (torch.device, optional): Target device to perform prediction on. Defaults to device.\n    \"\"\"\n\n    # Open image\n    img = Image.open(image_path)\n\n    # Create transformation for image (if one doesn't exist)\n    if transform is not None:\n        image_transform = transform\n    else:\n        image_transform = transforms.Compose(\n            [\n                transforms.Resize(image_size),\n                transforms.ToTensor(),\n                transforms.Normalize(\n                    mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]\n                ),\n            ]\n        )\n\n    ### Predict on image ###\n\n    # Make sure the model is on the target device\n    model.to(device)\n\n    # Turn on model evaluation mode and inference mode\n    model.eval()\n    with torch.inference_mode():\n        # Transform and add an extra dimension to image (model requires samples in [batch_size, color_channels, height, width])\n        transformed_image = image_transform(img).unsqueeze(dim=0)\n\n        # Make a prediction on image with an extra dimension and send it to the target device\n        target_image_pred = model(transformed_image.to(device))\n\n    # Convert logits -> prediction probabilities (using torch.softmax() for multi-class classification)\n    target_image_pred_probs = torch.softmax(target_image_pred, dim=1)\n\n    # Convert prediction probabilities -> prediction labels\n    target_image_pred_label = torch.argmax(target_image_pred_probs, dim=1)\n\n    # Plot image with predicted label and probability\n    plt.figure()\n    plt.imshow(img)\n    plt.title(\n        f\"Pred: {class_names[target_image_pred_label]} | Prob: {target_image_pred_probs.max():.3f}\"\n    )\n    plt.axis(False)\n","metadata":{"id":"DBWiMIoZXjX2","execution":{"iopub.status.busy":"2024-10-13T23:13:57.154484Z","iopub.execute_input":"2024-10-13T23:13:57.154708Z","iopub.status.idle":"2024-10-13T23:13:57.163186Z","shell.execute_reply.started":"2024-10-13T23:13:57.154686Z","shell.execute_reply":"2024-10-13T23:13:57.162576Z"},"trusted":true},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"# **Pre-training**","metadata":{"id":"NVc7DBqjuIGh"}},{"cell_type":"code","source":"# 1. Get pretrained weights for ViT-Huge\npretrained_vit_weights = torchvision.models.ViT_H_14_Weights.DEFAULT\n\n# 2. Setup a ViT model instance with pretrained weights\npretrained_vit = torchvision.models.vit_h_14(weights=pretrained_vit_weights).to(device)\n\n# 3. Freeze the base parameters\nfor parameter in pretrained_vit.parameters():\n    parameter.requires_grad = False\n\n# 4. Change the classifier head\nclass_names = ['burger','butter_naan','chai','chapati','chole_bhature','dal_makhani','dhokla','fried_rice','idli','jalebi','21-kaathi_rolls','kadai_paneer','kulfi','masala_dosa','momos','paani_puri','pakode','pav_bhaji','pizza','samosa']\n\nset_seeds()\npretrained_vit.heads = nn.Linear(in_features=1280, out_features=len(class_names)).to(device)\n# pretrained_vit # uncomment for model output","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WCTBI1htWkQm","outputId":"5846cb36-fefb-42b3-856b-896e3d936402","execution":{"iopub.status.busy":"2024-10-13T23:13:57.163942Z","iopub.execute_input":"2024-10-13T23:13:57.164162Z","iopub.status.idle":"2024-10-13T23:14:19.622086Z","shell.execute_reply.started":"2024-10-13T23:13:57.164124Z","shell.execute_reply":"2024-10-13T23:14:19.621348Z"},"trusted":true},"outputs":[{"name":"stderr","text":"Downloading: \"https://download.pytorch.org/models/vit_h_14_swag-80465313.pth\" to /root/.cache/torch/hub/checkpoints/vit_h_14_swag-80465313.pth\n100%|██████████| 2.36G/2.36G [00:11<00:00, 211MB/s]\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from torchinfo import summary\n\n# Print a summary using torchinfo (uncomment for actual output)\nsummary(model=pretrained_vit,\n        input_size=(14, 3, 518, 518), # (batch_size, color_channels, height, width)\n        # col_names=[\"input_size\"], # uncomment for smaller output\n        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n        col_width=20,\n        row_settings=[\"var_names\"])","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kog2q8aTWqnT","outputId":"f3aab87d-bba7-43a0-b736-be3a8c29b725","execution":{"iopub.status.busy":"2024-10-13T23:14:19.623029Z","iopub.execute_input":"2024-10-13T23:14:19.623297Z","iopub.status.idle":"2024-10-13T23:14:51.628576Z","shell.execute_reply.started":"2024-10-13T23:14:19.623272Z","shell.execute_reply":"2024-10-13T23:14:51.627908Z"},"trusted":true},"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"============================================================================================================================================\nLayer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n============================================================================================================================================\nVisionTransformer (VisionTransformer)                        [14, 3, 518, 518]    [14, 20]             1,280                Partial\n├─Conv2d (conv_proj)                                         [14, 3, 518, 518]    [14, 1280, 37, 37]   (753,920)            False\n├─Encoder (encoder)                                          [14, 1370, 1280]     [14, 1370, 1280]     1,753,600            False\n│    └─Dropout (dropout)                                     [14, 1370, 1280]     [14, 1370, 1280]     --                   --\n│    └─Sequential (layers)                                   [14, 1370, 1280]     [14, 1370, 1280]     --                   False\n│    │    └─EncoderBlock (encoder_layer_0)                   [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_1)                   [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_2)                   [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_3)                   [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_4)                   [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_5)                   [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_6)                   [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_7)                   [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_8)                   [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_9)                   [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_10)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_11)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_12)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_13)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_14)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_15)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_16)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_17)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_18)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_19)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_20)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_21)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_22)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_23)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_24)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_25)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_26)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_27)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_28)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_29)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_30)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    │    └─EncoderBlock (encoder_layer_31)                  [14, 1370, 1280]     [14, 1370, 1280]     (19,677,440)         False\n│    └─LayerNorm (ln)                                        [14, 1370, 1280]     [14, 1370, 1280]     (2,560)              False\n├─Linear (heads)                                             [14, 1280]           [14, 20]             25,620               True\n============================================================================================================================================\nTotal params: 632,215,060\nTrainable params: 25,620\nNon-trainable params: 632,189,440\nTotal mult-adds (G): 20.33\n============================================================================================================================================\nInput size (MB): 45.08\nForward/backward pass size (MB): 44386.98\nParams size (MB): 1682.32\nEstimated Total Size (MB): 46114.39\n============================================================================================================================================"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Get automatic transforms from pretrained ViT weights\npretrained_vit_transforms = pretrained_vit_weights.transforms()\nprint(pretrained_vit_transforms)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ml8CGo3bXZq3","outputId":"ffebe995-7838-4705-c46f-655c514aff53","execution":{"iopub.status.busy":"2024-10-13T23:14:51.630852Z","iopub.execute_input":"2024-10-13T23:14:51.631175Z","iopub.status.idle":"2024-10-13T23:14:51.635307Z","shell.execute_reply.started":"2024-10-13T23:14:51.631118Z","shell.execute_reply":"2024-10-13T23:14:51.634631Z"},"trusted":true},"outputs":[{"name":"stdout","text":"ImageClassification(\n    crop_size=[518]\n    resize_size=[518]\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n    interpolation=InterpolationMode.BICUBIC\n)\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import os\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\nNUM_WORKERS = os.cpu_count()\n\ndef create_dataloaders(\n    train_dir: str,\n    test_dir: str,\n    transform: transforms.Compose,\n    batch_size: int,\n    num_workers: int=NUM_WORKERS\n):\n\n  # Use ImageFolder to create dataset(s)\n  train_data = datasets.ImageFolder(train_dir, transform=transform)\n  test_data = datasets.ImageFolder(test_dir, transform=transform)\n\n  # Get class names\n  class_names = train_data.classes\n\n  # Turn images into data loaders\n  train_dataloader = DataLoader(\n      train_data,\n      batch_size=batch_size,\n      shuffle=True,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n  test_dataloader = DataLoader(\n      test_data,\n      batch_size=batch_size,\n      shuffle=False,\n      num_workers=num_workers,\n      pin_memory=True,\n  )\n\n  return train_dataloader, test_dataloader, class_names\n","metadata":{"id":"mYjRl30NXal3","execution":{"iopub.status.busy":"2024-10-13T23:14:51.636064Z","iopub.execute_input":"2024-10-13T23:14:51.636314Z","iopub.status.idle":"2024-10-13T23:14:51.646236Z","shell.execute_reply.started":"2024-10-13T23:14:51.636290Z","shell.execute_reply":"2024-10-13T23:14:51.645642Z"},"trusted":true},"outputs":[],"execution_count":12},{"cell_type":"code","source":"# Setup dataloaders\ntrain_dataloader_pretrained, test_dataloader_pretrained, class_names = create_dataloaders(train_dir=train_dir,\n                                                                                                     test_dir=test_dir,\n                                                                                                     transform=pretrained_vit_transforms,\n                                                                                                     batch_size=14) # Could increase if we had more samples, such as here: https://arxiv.org/abs/2205.01580 (there are other improvements there too...)","metadata":{"id":"6omzlOFIY14u","execution":{"iopub.status.busy":"2024-10-13T23:14:51.646917Z","iopub.execute_input":"2024-10-13T23:14:51.647125Z","iopub.status.idle":"2024-10-13T23:14:53.371267Z","shell.execute_reply.started":"2024-10-13T23:14:51.647103Z","shell.execute_reply":"2024-10-13T23:14:53.370415Z"},"trusted":true},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Create optimizer and loss function\noptimizer = torch.optim.Adam(params=pretrained_vit.parameters(),\n                             lr=1e-3)\nloss_fn = torch.nn.CrossEntropyLoss()\n\n# Train the classifier head of the pretrained ViT feature extractor model\nset_seeds()\npretrained_vit_results = en_train(model=pretrained_vit,\n                                      train_dataloader=train_dataloader_pretrained,\n                                      test_dataloader=test_dataloader_pretrained,\n                                      optimizer=optimizer,\n                                      loss_fn=loss_fn,\n                                      epochs=10,\n                                      device=device)","metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","referenced_widgets":["45435f8d207c44c9b3611351ba4ee55f","e69e159c8d8b46cab71ff1c42db3c385","7214f669b7d64e039033850ad9012e1a","29268cbbf3484e21a37bf65de4f6c4ee","e44b6bcef4364403aef894d9b21bfb9f","6da04b76addd42a4b4044955559d992f","e3d6b8cf2ef5410690f6a8c9c04221c7","2b21ef9a47c84604a9c7d118332ccf34","e0628ca05aa94710916516a9bc024b75","ef81085bff5c455c87415289f73b54f9","4936affa4d894df3b4afaf06142dd972"]},"id":"ARp9mqRWXeIh","outputId":"4bab7d95-06a2-418d-a6da-f0dac4a84cbf","execution":{"iopub.status.busy":"2024-10-13T23:14:53.372674Z","iopub.execute_input":"2024-10-13T23:14:53.372939Z","iopub.status.idle":"2024-10-14T01:10:56.186990Z","shell.execute_reply.started":"2024-10-13T23:14:53.372913Z","shell.execute_reply":"2024-10-14T01:10:56.185705Z"},"trusted":true},"outputs":[{"name":"stderr","text":"  0%|          | 0/10 [1:56:01<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Train the classifier head of the pretrained ViT feature extractor model\u001b[39;00m\n\u001b[1;32m      7\u001b[0m set_seeds()\n\u001b[0;32m----> 8\u001b[0m pretrained_vit_results \u001b[38;5;241m=\u001b[39m \u001b[43men_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_vit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader_pretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mtest_dataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest_dataloader_pretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[7], line 169\u001b[0m, in \u001b[0;36men_train\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# Loop through training and testing steps for a number of epochs\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28mrange\u001b[39m(epochs)):\n\u001b[0;32m--> 169\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     test_loss, test_acc \u001b[38;5;241m=\u001b[39m test_step(model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    175\u001b[0m       dataloader\u001b[38;5;241m=\u001b[39mtest_dataloader,\n\u001b[1;32m    176\u001b[0m       loss_fn\u001b[38;5;241m=\u001b[39mloss_fn,\n\u001b[1;32m    177\u001b[0m       device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    179\u001b[0m     \u001b[38;5;66;03m# Print out what's happening\u001b[39;00m\n","Cell \u001b[0;32mIn[7], line 45\u001b[0m, in \u001b[0;36mtrain_step\u001b[0;34m(model, dataloader, loss_fn, optimizer, device)\u001b[0m\n\u001b[1;32m     42\u001b[0m X, y \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mto(device), y\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# 1. Forward pass\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# 2. Calculate  and accumulate loss\u001b[39;00m\n\u001b[1;32m     48\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred, y)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/models/vision_transformer.py:298\u001b[0m, in \u001b[0;36mVisionTransformer.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    295\u001b[0m batch_class_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclass_token\u001b[38;5;241m.\u001b[39mexpand(n, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    296\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([batch_class_token, x], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 298\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;66;03m# Classifier \"token\" as used by standard language architectures\u001b[39;00m\n\u001b[1;32m    301\u001b[0m x \u001b[38;5;241m=\u001b[39m x[:, \u001b[38;5;241m0\u001b[39m]\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/models/vision_transformer.py:157\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    155\u001b[0m torch\u001b[38;5;241m.\u001b[39m_assert(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected (batch_size, seq_length, hidden_dim) got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos_embedding\n\u001b[0;32m--> 157\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torchvision/models/vision_transformer.py:113\u001b[0m, in \u001b[0;36mEncoderBlock.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    111\u001b[0m torch\u001b[38;5;241m.\u001b[39m_assert(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected (batch_size, seq_length, hidden_dim) got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    112\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mln_1(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m--> 113\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mself_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(x)\n\u001b[1;32m    115\u001b[0m x \u001b[38;5;241m=\u001b[39m x \u001b[38;5;241m+\u001b[39m \u001b[38;5;28minput\u001b[39m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/modules/activation.py:1275\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1261\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[1;32m   1262\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[1;32m   1263\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1272\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[1;32m   1273\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[1;32m   1274\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1275\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[1;32m   1287\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n","File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch/nn/functional.py:5560\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   5557\u001b[0m k \u001b[38;5;241m=\u001b[39m k\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[1;32m   5558\u001b[0m v \u001b[38;5;241m=\u001b[39m v\u001b[38;5;241m.\u001b[39mview(bsz, num_heads, src_len, head_dim)\n\u001b[0;32m-> 5560\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m \u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5561\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m attn_output\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\u001b[38;5;241m.\u001b[39mcontiguous()\u001b[38;5;241m.\u001b[39mview(bsz \u001b[38;5;241m*\u001b[39m tgt_len, embed_dim)\n\u001b[1;32m   5563\u001b[0m attn_output \u001b[38;5;241m=\u001b[39m linear(attn_output, out_proj_weight, out_proj_bias)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}],"execution_count":14},{"cell_type":"code","source":"# Plot the loss curves\nplot_loss_curves(pretrained_vit_results)","metadata":{"id":"6eJRpVY8XhN1","execution":{"iopub.status.busy":"2024-10-14T01:10:56.187754Z","iopub.status.idle":"2024-10-14T01:10:56.188058Z","shell.execute_reply.started":"2024-10-14T01:10:56.187903Z","shell.execute_reply":"2024-10-14T01:10:56.187918Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# CONFUSION MATRIX\n\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\n# Step 1: Get all true labels and predictions from the test dataset\nall_labels = []\nall_preds = []\n\npretrained_vit.eval()  # Set the model to evaluation mode\nwith torch.inference_mode():\n    for images, labels in test_dataloader_pretrained:\n        images, labels = images.to(device), labels.to(device)\n        outputs = pretrained_vit(images)\n        _, preds = torch.max(outputs, 1)\n        all_labels.extend(labels.cpu().numpy())\n        all_preds.extend(preds.cpu().numpy())\n\n# Step 2: Create the confusion matrix\n# cm = confusion_matrix(all_labels, all_preds)\n\n# # Step 3: Plot the confusion matrix\n# disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n# disp.plot(cmap=plt.cm.Blues, values_format='d')\n\n\n# plt.xticks(rotation=45)\n# plt.show()\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\n\n# Create the confusion matrix\ncm = confusion_matrix(all_labels, all_preds)\n\n# Plot the confusion matrix\nplt.figure(figsize=(20, 16))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\nplt.xticks(rotation=90, ha='right')\nplt.yticks(rotation=0)\nplt.tight_layout()\nplt.show()","metadata":{"id":"kTIH_js9XEtH","execution":{"iopub.status.busy":"2024-10-14T01:10:56.188842Z","iopub.status.idle":"2024-10-14T01:10:56.189122Z","shell.execute_reply.started":"2024-10-14T01:10:56.188985Z","shell.execute_reply":"2024-10-14T01:10:56.188999Z"},"trusted":true},"outputs":[],"execution_count":null}]}